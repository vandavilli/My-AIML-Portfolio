{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing lemma because its important for interpreting adverse drug events accurately.\n",
    "def lemmatizer(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    lemmatized_words = [lemma.lemmatize(word) for word in words \n",
    "                        if word.lower() not in stop_words and word not in punctuation]\n",
    "\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_lemmatized_tokens = negative_df['context_clean'].apply(lemmatizer)\n",
    "positive_lemmatized_tokens = positive_df['context_clean'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b067f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {'Adverse_event': 1, 'Potential_therapeutic_event': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da354e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['event_type'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1762253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mesg(message):\n",
    "    message = message.lower()\n",
    "    message = re.sub(r'[^a-zA-Z0-9\\s]', ' ', message)\n",
    "    words = word_tokenize(message)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    preprocessed_words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords_set]\n",
    "    cleaned_message = \" \".join(preprocessed_words)\n",
    "    return cleaned_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from CSV files\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "dev_data = pd.read_csv('data/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80796ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {'Adverse_event': 1, 'Potential_therapeutic_event': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c318d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentiment'] = train_data['event_type'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split the data into features (X) and target (y)\n",
    "X_train = train_data.drop('sentiment', axis=1)  # Replace 'target_column_name' with the actual name of your target column\n",
    "y_train = train_data['sentiment']\n",
    "\n",
    "# Split the test data into X_test and y_test\n",
    "X_test = test_data.drop('sentiment', axis=1)\n",
    "y_test = test_data['sentiment']\n",
    "\n",
    "# Split the dev data into X_dev and y_dev\n",
    "X_dev = dev_data.drop('sentiment', axis=1)\n",
    "y_dev = dev_data['sentiment']\n",
    "\n",
    "# Perform train-test split on the training data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Train set shape:\", X_train_final.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Dev set shape:\", X_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3cf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['context_clean'].values\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "y_pred = nb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "formatted_accuracy = \"{:.2%}\".format(accuracy)\n",
    "print(\"Accuracy:\", formatted_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(nb, X_train_tfidf)\n",
    "shap_values = explainer(X_test_tfidf)\n",
    "shap.summary_plot(shap_values, X_test_tfidf, feature_names=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85960c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    sentiment = classifier.predict(text_vectorized)[0]\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This drug is dangerous & creates high blood pressure.\"\n",
    "sentiment = predict_sentiment(text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This drug is very safe.\"\n",
    "sentiment = predict_sentiment(text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004abb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cad0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
