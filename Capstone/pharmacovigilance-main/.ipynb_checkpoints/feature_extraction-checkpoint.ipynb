{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "345877ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, Doc2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3f4b1",
   "metadata": {},
   "source": [
    "#### Load pre-trained word vectors from a binary file located at the specified path. The file contains word vectors in a format compatible with Word2Vec. Only the first 100,000 word vectors are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1242640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './biowordvec/BioWordVec_PubMed_MIMICIII_d200.vec.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "358a13eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/t1pgd71d3296txhmwy6fg76r0000gn/T/ipykernel_9236/727050312.py:1: DtypeWarning: Columns (112,114,121,125,157,168,186,188,206,208,217,229,231,232,233,235,237,241,244,246,247,249,250,251,268,270,271,280,282,286,288,289,291,305,310,312,313,315,316,318,319,323,324,327,328,329,331,333,335,336,338,342,344,348,350,352,353,355,356,358,359,361) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('data/train.csv')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "dev_data = pd.read_csv('data/dev.csv')\n",
    "\n",
    "data = pd.concat([train_data, test_data, dev_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1951798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected columns to be retained and rename them\n",
    "selected_columns = ['id', 'context', 'annotations/0/events/0/event_type', \n",
    "                    'annotations/0/events/0/Trigger/text/0/0', \n",
    "                    'annotations/0/events/0/Treatment/Drug/text/0/0','annotations/0/events/0/Effect/text/0/0']\n",
    "new_names = ['id', 'context', 'event_type', 'trigger_text', 'drug', 'drug_effect']\n",
    "df = data[selected_columns].rename(columns=dict(zip(selected_columns, new_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542a829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {'Adverse_event': 1, 'Potential_therapeutic_event': 0}\n",
    "df['sentiment'] = df['event_type'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146bfa2",
   "metadata": {},
   "source": [
    "#### Used instead of TF-IDF as its a more prevalent technique in sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f83a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_embeddings(df, column, word_embeddings):\n",
    "    embeddings = []\n",
    "    for document in df[column]:\n",
    "        for word in document.split():\n",
    "            if word in word_embeddings:\n",
    "                embeddings.append(word_embeddings[word])\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros_like(word_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc5febe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_embeddings'] = df.apply(lambda row: average_word_embeddings(row, 'context', model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc465a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('phamacovigil_output_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56539053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>event_type</th>\n",
       "      <th>trigger_text</th>\n",
       "      <th>drug</th>\n",
       "      <th>drug_effect</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>average_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>3628148_1</td>\n",
       "      <td>Rupture of a cerebral aneurysm associated with...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>associated</td>\n",
       "      <td>nifedipine</td>\n",
       "      <td>Rupture of a cerebral aneurysm</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.24257979, 0.15962923, 0.12351615, 0.0553195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>17763133_1</td>\n",
       "      <td>Cyclosporine is a potent inhibitor of simvasta...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>facilitate</td>\n",
       "      <td>Cyclosporine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.26850358, 0.2159426, 0.16674991, -0.0132749...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>18472517_2</td>\n",
       "      <td>We present here a female patient who developed...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>developed</td>\n",
       "      <td>enalaprilat</td>\n",
       "      <td>acute bilateral parotitis</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2470092, 0.18956128, 0.12259149, 0.06029636...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2738729_2</td>\n",
       "      <td>Withdrawal emergent syndrome in an infant asso...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>associated</td>\n",
       "      <td>haloperidol</td>\n",
       "      <td>Withdrawal emergent syndrome</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.20956247, 0.18645857, 0.11164258, 0.0011130...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>7900744_1</td>\n",
       "      <td>Case report: mannitol nephrotoxicity syndrome:...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>syndrome</td>\n",
       "      <td>mannitol</td>\n",
       "      <td>nephrotoxicity</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.23208107, 0.16797915, 0.19282238, 0.0063195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>16620273_2</td>\n",
       "      <td>Severe acidosis in patients taking metformin--...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>taking</td>\n",
       "      <td>metformin</td>\n",
       "      <td>Severe acidosis</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2167551, 0.1612138, 0.13051808, -0.01099174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>3718111_1</td>\n",
       "      <td>He became hyperkalemic on rechallenge with tim...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>following</td>\n",
       "      <td>timolol</td>\n",
       "      <td>hyperkalemic</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.23711173, 0.12776645, 0.120562576, -0.01220...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>15482394_1</td>\n",
       "      <td>An objective causality assessment indicated a ...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>relationship</td>\n",
       "      <td>warfarin</td>\n",
       "      <td>clotting abnormality</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2654402, 0.18026303, 0.14470221, 0.01915568...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>18160579_1</td>\n",
       "      <td>Pericardial hemorrhage due to acetylsalicylic ...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>due</td>\n",
       "      <td>acetylsalicylic acid</td>\n",
       "      <td>Pericardial hemorrhage</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.27963316, 0.15577544, 0.108165085, 0.000977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>2719905_1</td>\n",
       "      <td>Long lasting respiratory depression induced by...</td>\n",
       "      <td>Adverse_event</td>\n",
       "      <td>induced</td>\n",
       "      <td>morphine-6-glucuronide</td>\n",
       "      <td>Long lasting respiratory depression</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.21859325, 0.080682576, 0.16367914, -0.02550...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                            context  \\\n",
       "904    3628148_1  Rupture of a cerebral aneurysm associated with...   \n",
       "4422  17763133_1  Cyclosporine is a potent inhibitor of simvasta...   \n",
       "4276  18472517_2  We present here a female patient who developed...   \n",
       "170    2738729_2  Withdrawal emergent syndrome in an infant asso...   \n",
       "742    7900744_1  Case report: mannitol nephrotoxicity syndrome:...   \n",
       "3674  16620273_2  Severe acidosis in patients taking metformin--...   \n",
       "3956   3718111_1  He became hyperkalemic on rechallenge with tim...   \n",
       "2130  15482394_1  An objective causality assessment indicated a ...   \n",
       "4316  18160579_1  Pericardial hemorrhage due to acetylsalicylic ...   \n",
       "3663   2719905_1  Long lasting respiratory depression induced by...   \n",
       "\n",
       "         event_type  trigger_text                    drug  \\\n",
       "904   Adverse_event    associated              nifedipine   \n",
       "4422  Adverse_event    facilitate            Cyclosporine   \n",
       "4276  Adverse_event     developed             enalaprilat   \n",
       "170   Adverse_event    associated             haloperidol   \n",
       "742   Adverse_event      syndrome                mannitol   \n",
       "3674  Adverse_event        taking               metformin   \n",
       "3956  Adverse_event     following                 timolol   \n",
       "2130  Adverse_event  relationship                warfarin   \n",
       "4316  Adverse_event           due    acetylsalicylic acid   \n",
       "3663  Adverse_event       induced  morphine-6-glucuronide   \n",
       "\n",
       "                              drug_effect  sentiment  \\\n",
       "904        Rupture of a cerebral aneurysm          1   \n",
       "4422                                  NaN          1   \n",
       "4276            acute bilateral parotitis          1   \n",
       "170          Withdrawal emergent syndrome          1   \n",
       "742                        nephrotoxicity          1   \n",
       "3674                      Severe acidosis          1   \n",
       "3956                         hyperkalemic          1   \n",
       "2130                 clotting abnormality          1   \n",
       "4316               Pericardial hemorrhage          1   \n",
       "3663  Long lasting respiratory depression          1   \n",
       "\n",
       "                                     average_embeddings  \n",
       "904   [0.24257979, 0.15962923, 0.12351615, 0.0553195...  \n",
       "4422  [0.26850358, 0.2159426, 0.16674991, -0.0132749...  \n",
       "4276  [0.2470092, 0.18956128, 0.12259149, 0.06029636...  \n",
       "170   [0.20956247, 0.18645857, 0.11164258, 0.0011130...  \n",
       "742   [0.23208107, 0.16797915, 0.19282238, 0.0063195...  \n",
       "3674  [0.2167551, 0.1612138, 0.13051808, -0.01099174...  \n",
       "3956  [0.23711173, 0.12776645, 0.120562576, -0.01220...  \n",
       "2130  [0.2654402, 0.18026303, 0.14470221, 0.01915568...  \n",
       "4316  [0.27963316, 0.15577544, 0.108165085, 0.000977...  \n",
       "3663  [0.21859325, 0.080682576, 0.16367914, -0.02550...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7a7e8",
   "metadata": {},
   "source": [
    "#### Fit a MultinomialNB & Print the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aa8ec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9078674948240165\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        89\n",
      "           1       0.91      1.00      0.95       877\n",
      "\n",
      "    accuracy                           0.91       966\n",
      "   macro avg       0.45      0.50      0.48       966\n",
      "weighted avg       0.82      0.91      0.86       966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('sentiment', axis=1),\n",
    "                                                    df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_embeddings = np.array(X_train['average_embeddings'].tolist())\n",
    "X_test_embeddings = np.array(X_test['average_embeddings'].tolist())\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_embeddings)\n",
    "X_test_scaled = scaler.transform(X_test_embeddings)\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = nb_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30695247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class precision recall f1-score support\n",
       "0     0      0.00   0.00     0.00      89\n",
       "1     1      0.91   1.00     0.95     877"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = report.split('\\n')\n",
    "data = [line.split() for line in lines[2:-5]]\n",
    "columns = ['class', 'precision', 'recall', 'f1-score', 'support']\n",
    "df_report = pd.DataFrame(data, columns=columns)\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03789171",
   "metadata": {},
   "source": [
    "### To be updated\n",
    "\n",
    "In the classification report DataFrame, the class labels are represented as `0` and `1`. According to your statement, Class `0` corresponds to PTE (Potential Therapeutic Event), and Class `1` corresponds to ADE (Adverse Event). \n",
    "\n",
    "Let's interpret the results for each class:\n",
    "\n",
    "- For Class `0` (PTE):\n",
    "  - Precision: The precision for PTE is `0.00`. This indicates that among the instances predicted as PTE, none of them are true positives.\n",
    "  - Recall: The recall for PTE is `0.00`. This means that none of the actual PTE instances are correctly predicted as PTE.\n",
    "  - F1-score: The F1-score for PTE is `0.00`. It is the harmonic mean of precision and recall, which is also low in this case.\n",
    "  - Support: The support for PTE is `89`. This represents the number of instances in the test data that belong to the PTE class.\n",
    "\n",
    "- For Class `1` (ADE):\n",
    "  - Precision: The precision for ADE is `0.91`. This indicates that among the instances predicted as ADE, 91% of them are true positives.\n",
    "  - Recall: The recall for ADE is `1.00`. This means that all the actual ADE instances are correctly predicted as ADE.\n",
    "  - F1-score: The F1-score for ADE is `0.95`. It is a measure of the model's accuracy in identifying ADE instances, combining precision and recall into a single metric.\n",
    "  - Support: The support for ADE is `877`. This represents the number of instances in the test data that belong to the ADE class.\n",
    "\n",
    "Based on these results, it seems that the model performs well in predicting ADE instances (Class `1`), achieving high precision, recall, and F1-score. However, it fails to correctly identify PTE instances (Class `0`), resulting in low performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46849a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/vandavilli/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_embeddings = np.array(X_train['average_embeddings'].tolist())\n",
    "X_test_embeddings = np.array(X_test['average_embeddings'].tolist())\n",
    "\n",
    "X_train_embeddings = np.abs(X_train_embeddings)\n",
    "X_test_embeddings = np.abs(X_test_embeddings)\n",
    "\n",
    "nmf = NMF(init='nndsvda', max_iter=1000)\n",
    "lda = LatentDirichletAllocation()\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler(feature_range=(0, 1))),\n",
    "    ('abs_transform', FunctionTransformer(np.abs)),\n",
    "    ('nmf', nmf),\n",
    "    ('lda', lda),\n",
    "    ('nb_model', nb_model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'lda__n_components': [5, 10, 15],\n",
    "    'nb_model__alpha': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train_embeddings, y_train)\n",
    "\n",
    "y_pred = grid_search.predict(X_test_embeddings)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96856b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
