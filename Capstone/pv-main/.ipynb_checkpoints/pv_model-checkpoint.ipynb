{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345877ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec, Doc2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run common_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "dataframes = [train_data, test_data]\n",
    "df = process_load_data(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc59e31",
   "metadata": {},
   "source": [
    "#### Load pre-trained word vectors from a binary file located at the specified path. The file contains word vectors in a format compatible with Word2Vec. Only the first 100,000 word vectors are loaded ( due to memory constraints )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './biowordvec/BioWordVec_PubMed_MIMICIII_d200.vec.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_embeddings(df, column, word_embeddings):\n",
    "    embeddings = []\n",
    "    for document in df[column]:\n",
    "        for word in document.split():\n",
    "            if word in word_embeddings:\n",
    "                embeddings.append(word_embeddings[word])\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros_like(word_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5febe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_embeddings'] = df.apply(lambda row: average_word_embeddings(row, 'context', model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3028d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94e993",
   "metadata": {},
   "source": [
    "## Using MultinomialNB on a un-balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709adf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('sentiment', axis=1),\n",
    "                                                    df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_embeddings = np.array(X_train['average_embeddings'].tolist())\n",
    "X_test_embeddings = np.array(X_test['average_embeddings'].tolist())\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_embeddings)\n",
    "X_test_scaled = scaler.transform(X_test_embeddings)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler(feature_range=(0, 1))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__alpha': [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train_embeddings, y_train)\n",
    "\n",
    "best_unbalanced_model = grid_search.best_estimator_\n",
    "\n",
    "cv_scores = cross_val_score(best_unbalanced_model, X_train_scaled, y_train, cv=5)\n",
    "print(\"CV scores:\", cv_scores)\n",
    "print(\"Mean CV score:\", cv_scores.mean())\n",
    "\n",
    "y_pred = best_unbalanced_model.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb693ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3966cde",
   "metadata": {},
   "source": [
    "## Let's now predict using MultinomialNB - Before Balancing!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = best_unbalanced_model.predict_proba(X_test_scaled)[:,1]\n",
    "evaluate_model(y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9de4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = sum(y_test == y_pred)\n",
    "total_predictions = len(y_test)\n",
    "accuracy_percentage = (correct_predictions / total_predictions) * 100\n",
    "print(\"Accuracy percentage (for unbalanced):\", accuracy_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af555a",
   "metadata": {},
   "source": [
    "## Conclusion for MultinomialNB on a un-balanced dataset.\n",
    "\n",
    "**Cross-Validation Scores:**\n",
    "\n",
    "The scores range from around 0.909 to 0.911, with a mean of approximately 0.910. This implies that, on average, the model is correctly predicting the sentiment of about 91% of the text in the dataset, when tested on 5 different splits.\n",
    "\n",
    "While generally a good performance, in a healthcare practice where making a wrong prediction could have serious consequences, we might need an even higher accuracy.\n",
    "\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "- The top left cell (1) represents \"True Positives\" - these are the instances that were positive (PTE class) and were correctly identified as positive by the model.\n",
    "\n",
    "- The top right cell (69) represents \"False Negatives\" - these are the instances that were positive (PTE class), but were incorrectly identified as negative (ADE class) by the model.\n",
    "\n",
    "- The bottom left cell (7) represents \"False Positives\" - these are the instances that were negative (ADE class), but were incorrectly identified as positive (PTE class) by the model.\n",
    "\n",
    "- The bottom right cell (697) represents \"True Negatives\" - these are the instances that were negative (ADE class) and were correctly identified as negative by the model.\n",
    "\n",
    "From these numbers, it is clear that the model excels at identifying the negative class (ADE), with 697 correct predictions and only 7 incorrect ones. However, the model struggles with the positive class (PTE), correctly identifying only 1 instance and incorrectly classifying 69. These results can be explained because of the dataset has not been balanced and has a lot of negative signal due to a disprotionately high number of ADE sentiments. \n",
    "\n",
    "This suggests that the model could be improved by collecting more data for the positive class (PTE), or by using techniques to better handle imbalanced data if the PTE class is underrepresented in your dataset.\n",
    "\n",
    "The model currently excels at identifying negative sentiments (ADE class) but is having some difficulty correctly identifying positive sentiments (PTE class). The model's ability to correctly identify positive sentiments might potentially be improved by collecting more data about positive sentiments or by adjusting the model to better handle class imbalances. Despite this, the model is still correctly identifying the sentiment about 91% of the time across different tests, which is a strong result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523534f",
   "metadata": {},
   "source": [
    "## Address class imbalance to improve MultinomialNB performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd713825",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['average_embeddings'].tolist())\n",
    "y = df['sentiment']\n",
    "\n",
    "print(\"Before oversampling: \", Counter(y))\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "\n",
    "print(\"After oversampling: \", Counter(y_over))\n",
    "\n",
    "X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_over, y_over, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_over = scaler.fit_transform(X_train_over)\n",
    "X_test_over = scaler.transform(X_test_over)\n",
    "\n",
    "# And retrain model (from above ) on the balanced dataset\n",
    "grid_search.fit(X_train_over, y_train_over)\n",
    "\n",
    "best_balanced_model = grid_search.best_estimator_\n",
    "\n",
    "cv_scores = cross_val_score(best_balanced_model, X_over, y_over, cv=5)\n",
    "print(\"CV scores (balanced):\", cv_scores)\n",
    "print(\"Mean CV score (balanced):\", np.mean(cv_scores))\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred_over = best_balanced_model.predict(X_test_over)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test_over, y_pred_over)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b93f1",
   "metadata": {},
   "source": [
    "## Summary Notes for MultinomialNB Classfier for unbalanced & balanced datasets:\n",
    "\n",
    "Here's a comparison table of the results before and after applying the class balancing technique (SMOTE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6842b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Before SMOTE': [349, 3517, [0.9095, 0.9095, 0.9093, 0.9093, 0.9110], 0.9097, [[1, 69], [7, 697]]],\n",
    "    'After SMOTE': [3517, 3517, [0.6147, 0.6119, 0.6353, 0.5998, 0.6152], 0.6154, [[471, 214], [313, 409]]]\n",
    "}\n",
    "\n",
    "index = ['Minority class count', 'Majority class count', 'CV scores', 'Mean CV score', 'Confusion Matrix']\n",
    "\n",
    "summary_report = pd.DataFrame(data, index=index)\n",
    "summary_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884c29e",
   "metadata": {},
   "source": [
    "Before class balancing, the model showed a high mean CV score (**0.9097**), but the confusion matrix suggested that it wasn't performing well on the minority class (only 1 true positive instance).\n",
    "\n",
    "After applying SMOTE to balance the classes, the count of the minority class increased to match the majority class (from 349 to 3517). However, the mean cross-validation score decreased (from 0.9097 to 0.6154), which indicates that overall model performance was reduced. But the model's ability to predict the minority class improved significantly, as shown by the increased count of true positives in the confusion matrix (from 1 to 471).\n",
    "\n",
    "**Implication:**\n",
    "\n",
    "Before we addressed the class imbalance, the model was heavily biased towards the majority class (ADE), leading to a high overall accuracy, but it struggled to identify the minority class (PTE).\n",
    "\n",
    "After we balanced the classes , the model's overall accuracy decreased. However, its ability to correctly identify instances of the minority class (PTE) improved substantially. This suggests that the trade-off resulted in a model that is more useful in practice, as it is now more capable of identifying both PTE and ADE events, which was the goal of this analysis.\n",
    "\n",
    "It's important to note that the objective in pharmacovigilance sentiment analysis is not merely to maximize accuracy, but also to effectively identify both positive and negative events. After balancing the classes, the model is better equipped to do this, despite the decrease in overall accuracy.\n",
    "\n",
    "\n",
    "***NOTE: I have used only train + test datasets. I will be using the hold out data dev to see how the model is working on unseen data***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504c826",
   "metadata": {},
   "source": [
    "## Let's now predict using MultinomialNB - After Balancing!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e77025",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv('data/dev.csv')\n",
    "dataframes = [dev_data]\n",
    "dev_df = process_load_data(dataframes)\n",
    "dev_df['average_embeddings'] = dev_df.apply(lambda row: average_word_embeddings(row, 'context', model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8050cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cabc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_embeddings = np.array(dev_df['average_embeddings'].tolist())\n",
    "dev_scaled = scaler.transform(dev_embeddings)\n",
    "\n",
    "dev_pred = best_balanced_model.predict(dev_scaled)\n",
    "comparison = pd.DataFrame({'Actual': dev_df['sentiment'], 'Predicted': dev_pred})\n",
    "\n",
    "# Determine the number of matches\n",
    "matches = comparison[comparison['Actual'] == comparison['Predicted']].shape[0]\n",
    "\n",
    "# Compute the percentage of correct predictions\n",
    "accuracy_percentage = (matches / comparison.shape[0]) * 100\n",
    "\n",
    "print(f\"Out of {comparison.shape[0]} entries in the dev set, {matches} were correctly predicted by the model.\")\n",
    "print(f\"Accuracy percentage: {accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = best_balanced_model.predict_proba(dev_scaled)[:,1]\n",
    "evaluate_model(dev_df['sentiment'], dev_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935967ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baac9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2545bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d04013f",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d185d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('sentiment', axis=1),\n",
    "                                                    df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_embeddings = np.array(X_train['average_embeddings'].tolist())\n",
    "X_test_embeddings = np.array(X_test['average_embeddings'].tolist())\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_embeddings)\n",
    "X_test_scaled = scaler.transform(X_test_embeddings)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler(feature_range=(0, 1))),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1.0, 10.0],  # example parameter grid for SVC\n",
    "    'classifier__kernel': ['linear', 'rbf'],  # you can customize this to the kernels you want to tune\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred, zero_division=1)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
